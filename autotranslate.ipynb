{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2048e6466b0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "SEED = 2222\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(\"VnCoreNLP-master\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'put', 'the', 'dustpan', 'in', 'the', 'broom', 'closet']\n",
      "['Cuốn', 'sách', 'này', 'là', 'của', 'tôi', '.', 'Của', 'bạn', 'đâu', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "def tokenize_en(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_vi(text):\n",
    "    return [tok for tok in itertools.chain.from_iterable(annotator.tokenize(text))]\n",
    "\n",
    "text_en = 'Please put the dustpan in the broom closet'\n",
    "text_vi = 'Cuốn sách này là của tôi. Của bạn đâu?'\n",
    "print(tokenize_en(text_en))\n",
    "print(tokenize_vi(text_vi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_raw_dataset():\n",
    "    data_dir = \"\"\n",
    "    en_sents = open(data_dir + 'english.txt', \"r\",encoding=\"utf-8\" ).read().splitlines()\n",
    "    vi_sents = open(data_dir + 'vietnamese.txt', \"r\" ,encoding=\"utf-8\").read().splitlines()\n",
    "    return {\n",
    "        \"English\": [line for line in en_sents[:5000]],\n",
    "        \"Vietnamese\": [line for line in vi_sents[:5000]],\n",
    "    }\n",
    "raw_data = create_raw_dataset()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"Vietnamese\"])\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.125)\n",
    "\n",
    "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
    "test.to_json(\"test.json\", orient=\"records\", lines=True)\n",
    "val.to_json(\"val.json\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (en) vocabulary: 1515\n",
      "Unique tokens in target (vi) vocabulary: 1375\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "source_tokenizer = tokenize_en\n",
    "target_tokenizer = tokenize_vi\n",
    "\n",
    "def load_data(filename, source_tokenizer, target_tokenizer):\n",
    "    examples = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            src = source_tokenizer(example[\"English\"])\n",
    "            trg = target_tokenizer(example[\"Vietnamese\"])\n",
    "            examples.append((src, trg))\n",
    "    return examples\n",
    "\n",
    "train_examples = load_data(\"train.json\", source_tokenizer, target_tokenizer)\n",
    "val_examples = load_data(\"val.json\", source_tokenizer, target_tokenizer)\n",
    "test_examples = load_data(\"test.json\", source_tokenizer, target_tokenizer)\n",
    "\n",
    "def build_vocab(tokenized_sentences, max_size=None, min_freq=1):\n",
    "    word_counts = Counter(chain(*tokenized_sentences))\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "    if max_size is not None:\n",
    "        sorted_words = sorted_words[:max_size]\n",
    "    vocabulary = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "    for word, count in sorted_words:\n",
    "        if count >= min_freq and word not in vocabulary:\n",
    "            vocabulary[word] = len(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "source_sentences_train = [example[0] for example in train_examples]\n",
    "target_sentences_train = [example[1] for example in train_examples]\n",
    "source_vocab = build_vocab(source_sentences_train, max_size=10000, min_freq=2)\n",
    "target_vocab = build_vocab(target_sentences_train, max_size=10000, min_freq=2)\n",
    "\n",
    "print(f\"Unique tokens in source (en) vocabulary: {len(source_vocab)}\")\n",
    "print(f\"Unique tokens in target (vi) vocabulary: {len(target_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence lengths:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 29]\n",
      "Target sentence lengths:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29]\n",
      "Batch 0:\n",
      "Source sequences:\n",
      "torch.Size([4, 128])\n",
      "Target sequences:\n",
      "torch.Size([16, 128])\n",
      "tensor([209,   8, 118, 121,  65,   6,   5,  18,  88,  27, 324,  57,   6,  88,\n",
      "         18,  36,  57, 436,   6,  57, 203,   8,   8,  65,   8,  12,   8,   8,\n",
      "         85,  27,   8,   5,  28,  42,   8, 127,  27,   8,   6, 244,   8,   6,\n",
      "          8, 210,   5,  41,   5,  27,  53,   1, 103, 498,   8, 674,   6,   8,\n",
      "         28,  42,   8, 328,  27,  35,  88, 118, 871,  59,  65,  57, 240,  48,\n",
      "         25,  25,   8,  65,  28,   6,   1,   6, 360,  21,  57, 269, 201,   8,\n",
      "          5, 136,  28,   8, 503, 432,   8,  35, 114,   8,  35,   5, 213,  80,\n",
      "         54,  27,  36, 127,  18,   8,  28,   8,  28,   8,   8, 250, 312,   5,\n",
      "        447,  28,  12,  28,  85,  27,   5, 545,  57,  28,   5,  36,  28,   5,\n",
      "         12, 494])\n",
      "Batch 1:\n",
      "Source sequences:\n",
      "torch.Size([20, 104])\n",
      "Target sequences:\n",
      "torch.Size([24, 104])\n",
      "tensor([  43,  586,  728,   34,    1,  399,   29, 1209,   52,   17,    5,    7,\n",
      "          16,   47,  101,   56,    1,    5,   31,  157,  304,   16,   34,  504,\n",
      "         505,    9,   90,   51,  314,  105,   47,   26,   20,   96,    9,  260,\n",
      "           7,   34,   15,   14,  361,   15,   32,  297,   44,   29,   11,    9,\n",
      "           5,  361,   11,  359,   16,  105,   62,   55,   33,   45,  301,   31,\n",
      "          45,    1,  143,    7,   56,   58,   70,   10,  195,  423,  240,  913,\n",
      "          66,  241,   15,    6,    5,   32,   33,    1,  257,    9,   18,   28,\n",
      "          15,  342,   45,   34,   43,  381,   82,  829,   62,   11,  108,    5,\n",
      "           6,  134,   41,    7,   10,    7,  301,   54])\n",
      "Batch 2:\n",
      "Source sequences:\n",
      "torch.Size([11, 128])\n",
      "Target sequences:\n",
      "torch.Size([24, 128])\n",
      "tensor([   7,   26,    9,    1,  867,   29,   73,    5,  204,  435,  202,   48,\n",
      "           1,   24,   54,   75,   75,   45,   52,    5,   48,  119,   33,   23,\n",
      "          37,   47, 1110,   29,   48,    9, 1288,  230,  125,   10,    1,   34,\n",
      "           1,   71,    1,   79,    5,  368,   19,  732,  629,   40,   19, 1374,\n",
      "          17,  262,   46,   12,  231,   66,   29,    6,    5,  521,   19, 1071,\n",
      "           5,   45,   33,   31,  435,  100,  192,   40,    1,  608,    1,    6,\n",
      "          11,   12,  326,    7,   26,  356,   51,   37,   26,    5,    1,  549,\n",
      "          33,   86,  202,   10,   62,   44,  245,  622,   36,   21,  225,  531,\n",
      "          83,  739,   16,    9, 1304,   48,   26,   48,   37,  665,  332,  461,\n",
      "          40,   17,   37,    1,   71,   21,    7,  136,    1,  163,   48,   53,\n",
      "           1,   45,  203,  116,   29,  188,  221,    5])\n",
      "Batch 3:\n",
      "Source sequences:\n",
      "torch.Size([6, 128])\n",
      "Target sequences:\n",
      "torch.Size([16, 128])\n",
      "tensor([ 394,    1,   54,  164,    7,   37,  334,   20,    6,  296,   49,  307,\n",
      "           1,   35,    9,  229,    7,    8,  151,  202,    1,   42,   38,  427,\n",
      "         608,   67,  328,    1,  490,   42, 1051,   20,   98,    0,   51,  538,\n",
      "          48,   10,    1,    1, 1097,   61,    6,   36,    0,  161,   11,   13,\n",
      "          34,   76,    6,  728,    8,   63,   10,  895,   11,  155,   59,   82,\n",
      "         306,   35,   50,   37,   20,   21, 1319,   24,   10,  525,  592,   35,\n",
      "         331,  548,    6,    1,  963,   41,    9,   17,   53,   10,  117,  479,\n",
      "         104,   38,   54,  254,    7,    1,   12,   71,  625,  206,    1,  373,\n",
      "          26,   82,  130,    0,   23,   20,   30,   11,   30,  268,   54,   20,\n",
      "           1,   24,    6,  123,   17,  349,   92,  688,  269,   21,    7,   45,\n",
      "          11,    4,  760,   89,  135,    5,    1,   19])\n",
      "Batch 4:\n",
      "Source sequences:\n",
      "torch.Size([5, 128])\n",
      "Target sequences:\n",
      "torch.Size([16, 128])\n",
      "tensor([ 938,   94,    0,    0,    0,    0,    0,    0,   18,   54,    4,    0,\n",
      "          29,    1,    0,    4,    4,    0,   20,    0,    0,   11,    0,  967,\n",
      "          13,    0,    0,   13,    5,    0,   16,    4,    0, 1224,   94,    1,\n",
      "           0,    1,    0,  104,   36,    6, 1254,   76,    4,   13,   11,  699,\n",
      "           1,   13,    0,    0,   23,  174,   20,  178,    5,   11,    0,    0,\n",
      "           4,    0,    4,    7,   76,  303,  307,  288,   72, 1180,   50, 1221,\n",
      "         267,    7,    4,   85,   32,    1,  419,    1,    5,  200,    1,    6,\n",
      "           0,  283,   34,    1,   10,    0,    0,  152,    7,    0,    4,   70,\n",
      "         338,  634,   36,   74,   12,  728,   10,   62,  227,    1,    6,    0,\n",
      "           7,  493,   17,   13,    5,   41,   84,   54,    1,   48,  860,    1,\n",
      "           4,    8,  574,   14,   23,  882,    0,  267])\n",
      "Batch 5:\n",
      "Source sequences:\n",
      "torch.Size([8, 128])\n",
      "Target sequences:\n",
      "torch.Size([16, 128])\n",
      "tensor([  12,    1,   40,   14,   41,  296,   76,   20,   29,    4,   43,  150,\n",
      "           0,   38,   34,   12,  117,   19,   35,    0,   16,    0,    1,  807,\n",
      "        1080,   41,    0,  161,    5,    1,    0,  443,   29,   37,   99,    8,\n",
      "          11,   10,  573,   10,    0,  278,   38,    0,   13,  890,   15,    1,\n",
      "           5,   23, 1133,   11,   23,   18,   67,   76,   19,  162,  148,   44,\n",
      "          46,   41,  680,    8,   21,  272,    5,   24,    0,  125,   10,   18,\n",
      "         585,  306,   74,  227,    1,  239,   80,   18,  323,  745,   48,   50,\n",
      "           1,    5,    9,   53,   36,  684,  227,   19,  406,    0,   20,   67,\n",
      "          15,   38,   66,   28,  136,  229,   47,   15,  336,   12,    0,   33,\n",
      "          35,  382,  168,   48,  452,  253,  151,   30,   24,   10, 1006,  105,\n",
      "          70,    0,   63,    1,   38,   35,   20,   20])\n",
      "Batch 6:\n",
      "Source sequences:\n",
      "torch.Size([9, 128])\n",
      "Target sequences:\n",
      "torch.Size([24, 128])\n",
      "tensor([   0,   13,    0,   32,    9,   79,    1,   23,   26,   49,  170,    4,\n",
      "         288,    0,  148,   23,   73,   15,    4,    4,    4,   98,  388,  185,\n",
      "          43,    1,   18,   41,   84,   19,   11,   29,  609,    7,   11,  645,\n",
      "           0,   32,    0,    0, 1290,  303,   10,  901,    0,    0,    4,   13,\n",
      "           1, 1076,    6,  585,  549,  139,   20,   74,  150,    0,    4,    0,\n",
      "          29,  220,    1,  197,  800,   30,    0,   14,   14,   32,  532,  436,\n",
      "         604,  836,    5,  197,   43,    0,    0,  111,   21,   53,  112,    0,\n",
      "           8,   51,   11,    4,  179,   30,   97,    6, 1015,    1,   36,   61,\n",
      "           7,   48,   10,    0,   51,   23,  107,  125,   77,    5,   81,   51,\n",
      "          73,   50,   10,   39,   50,   97,  113,   96,  998,  778,   12,   18,\n",
      "         187,    1,   10,   85,   30,  373,    0,    1])\n",
      "Batch 7:\n",
      "Source sequences:\n",
      "torch.Size([7, 128])\n",
      "Target sequences:\n",
      "torch.Size([16, 128])\n",
      "tensor([   4,    0,    0,    0,    0,   54,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,   67,    0,   18,    0,    5,    0,\n",
      "           0,    4,    0,    0,    0,  148,    0,  316,   11,    0,   18,    4,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,   96,   36,  153,\n",
      "          73,    0,    0,    0,    4,  236,    0,  119,    4,   38,    0,    0,\n",
      "           0,  465,    1,   18,   24,    0,    0,    4,    0,    0,   67,   51,\n",
      "           8,    0,   32,   20,    7,  532,    0,    4,   13,    0,  566,    0,\n",
      "           6,    0,   38,    0,    0,    0,    0,   38,    0,    0,    6,    0,\n",
      "           0,    0,   63,    0,   14,    0,   41,  464,    7,   19,    0,    0,\n",
      "           0,    0,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          13,    0,   23,   89,  632, 1029,  548,    0])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_iterator(data, batch_size, source_vocab, target_vocab, device):\n",
    "    src_sents = [x[0] for x in data]\n",
    "    trg_sents = [x[1] for x in data]\n",
    "\n",
    "    # Sort the sentences by length for efficiency\n",
    "    sorted_indices = sorted(range(len(src_sents)), key=lambda i: len(src_sents[i]))\n",
    "\n",
    "    # Split the data into batches\n",
    "    batches = []\n",
    "    for i in range(0, len(src_sents), batch_size):\n",
    "        indices = sorted_indices[i:i+batch_size]\n",
    "        src_batch = [src_sents[j] for j in indices]\n",
    "        trg_batch = [trg_sents[j] for j in indices]\n",
    "        batch = {\"src\": src_batch, \"trg\": trg_batch}\n",
    "        batches.append(batch)\n",
    "\n",
    "    # Shuffle the batches\n",
    "    random.shuffle(batches)\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for batch in batches:\n",
    "        # Convert the sentences to sequences of indices\n",
    "        src_seqs = [torch.LongTensor([source_vocab[token] if token in source_vocab else source_vocab['<unk>'] for token in sent]) for sent in batch['src']]\n",
    "        trg_seqs = [torch.LongTensor([target_vocab[token] if token in target_vocab else target_vocab['<unk>'] for token in sent]) for sent in batch['trg']]\n",
    "\n",
    "        # Pad the sequences\n",
    "        src_seqs = torch.nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=source_vocab['<pad>']).to(device)\n",
    "        trg_seqs = torch.nn.utils.rnn.pad_sequence(trg_seqs, batch_first=True, padding_value=target_vocab['<pad>']).to(device)\n",
    "        trg_seqs = F.pad(trg_seqs, (0, 8 - (trg_seqs.size(1) % 8)), value=target_vocab['<pad>'])\n",
    "\n",
    "        # Transpose the source sequences tensor to have batch size as the first dimension\n",
    "        src_seqs = src_seqs.transpose(0, 1)\n",
    "        trg_seqs = trg_seqs.transpose(0, 1)\n",
    "\n",
    "        # Return the batch\n",
    "        yield {\"src\": src_seqs, \"trg\": trg_seqs}\n",
    "\n",
    "\n",
    "train_batches = get_iterator(train_examples, BATCH_SIZE, source_vocab, target_vocab, device)\n",
    "valid_batches = get_iterator(val_examples, BATCH_SIZE, source_vocab, target_vocab, device)\n",
    "test_batches = get_iterator(test_examples, BATCH_SIZE, source_vocab, target_vocab, device)\n",
    "source_lengths = [len(sent) for sent in source_sentences_train]\n",
    "target_lengths = [len(sent) for sent in target_sentences_train]\n",
    "print(\"Source sentence lengths: \", sorted(set(source_lengths)))\n",
    "print(\"Target sentence lengths: \", sorted(set(target_lengths)))\n",
    "\n",
    "for i, batch in enumerate (test_batches):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(\"Source sequences:\")\n",
    "    print(batch[\"src\"].shape)\n",
    "    print(\"Target sequences:\")\n",
    "    print(batch[\"trg\"].shape)\n",
    "    print(batch[\"trg\"][i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128, 1024]) torch.Size([128, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# adjustable parameters\n",
    "INPUT_DIM = len(source_vocab)\n",
    "OUTPUT_DIM = len(target_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, n_layers, dropout=dropout,\n",
    "                          bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "    def forward(self, src_batch):\n",
    "        # src [sent len, batch size]\n",
    "\n",
    "        # [sent len, batch size, emb dim]\n",
    "        embedded = self.embedding(src_batch)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        # hidden -> [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        # initial decoder hidden is final hidden state of the forwards and\n",
    "        # backwards encoder RNNs fed through a linear layer\n",
    "        concated = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        hidden = torch.tanh(self.fc(concated))\n",
    "        return outputs, hidden\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "outputs, hidden = encoder(batch[\"src\"])\n",
    "# transpose the outputs tensor to have batch size as the first dimension\n",
    "print(outputs.shape, hidden.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 7])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        # enc_hid_dim multiply by 2 due to bidirectional\n",
    "        self.fc1 = nn.Linear(enc_hid_dim * 2 + dec_hid_dim, dec_hid_dim)\n",
    "        self.fc2 = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        \n",
    "        # repeat encoder hidden state src_len times [batch size, sent len, dec hid dim]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # reshape/permute the encoder output, so that the batch size comes first\n",
    "        # [batch size, sent len, enc hid dim * 2], times 2 because of bidirectional\n",
    "        outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # the attention mechanism receives a concatenation of the hidden state\n",
    "        # and the encoder output\n",
    "        concat = torch.cat((hidden, outputs), dim=2)\n",
    "        \n",
    "        # fully connected layer and softmax layer to compute the attention weight\n",
    "        # [batch size, sent len, dec hid dim]\n",
    "        energy = torch.tanh(self.fc1(concat))\n",
    "        # attention weight should be of [batch size, sent len]\n",
    "        attention = self.fc2(energy).squeeze(dim=2)  \n",
    "        attention_weight = torch.softmax(attention, dim=1)\n",
    "        return attention_weight\n",
    "\n",
    "    \n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM).to(device)\n",
    "attention_weight = attention(outputs, hidden)\n",
    "attention_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1375]), torch.Size([128, 512]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers,\n",
    "                 dropout, attention):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hid_dim * 2 + emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(dec_hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, trg, encoder_outputs, hidden):\n",
    "        # trg [batch size]\n",
    "        # outputs [src sen len, batch size, enc hid dim * 2], times 2 due to bidirectional\n",
    "        # hidden [batch size, dec hid dim]\n",
    "\n",
    "        # [batch size, 1, sent len] \n",
    "        attention = self.attention(encoder_outputs, hidden).unsqueeze(1)\n",
    "\n",
    "        # [batch size, sent len, enc hid dim * 2]\n",
    "        outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # [1, batch size, enc hid dim * 2]\n",
    "        context = torch.bmm(attention, outputs).permute(1, 0, 2)\n",
    "\n",
    "        # input sentence -> embedding\n",
    "        # [1, batch size, emb dim]\n",
    "        embedded = self.embedding(trg.unsqueeze(0))\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        outputs, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        prediction = self.linear(outputs.squeeze(0))\n",
    "        return prediction, hidden.squeeze(0)\n",
    "\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attention).to(device)\n",
    "prediction, decoder_hidden = decoder(batch[\"trg\"][0], outputs, hidden)\n",
    "\n",
    "# notice the decoder_hidden's shape should match the shape that's generated by\n",
    "# the encoder\n",
    "prediction.shape, decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1515, 256)\n",
       "    (rnn): GRU(256, 512, dropout=0.5, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (fc1): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(1375, 256)\n",
       "    (rnn): GRU(1280, 512, dropout=0.5)\n",
       "    (linear): Linear(in_features=512, out_features=1375, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src_batch, trg_batch, teacher_forcing_ratio=0.5):\n",
    "        max_len, batch_size = trg_batch.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # encoder_outputs : all hidden states of the input sequence (forward and backward)\n",
    "        # hidden : final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src_batch)\n",
    "\n",
    "        trg = trg_batch[0]\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden = self.decoder(trg, encoder_outputs, hidden)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = trg_batch[i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "attention = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attention)\n",
    "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 1375])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = seq2seq(batch[\"src\"], batch[\"trg\"])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,878,495 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq.parameters())\n",
    "\n",
    "# ignore the padding index when calculating the loss\n",
    "PAD_IDX = target_vocab['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    seq2seq.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch.src, batch.trg)\n",
    "\n",
    "        # the loss function only works on 2d inputs\n",
    "        # and 1d targets we need to flatten each of them\n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch.trg[1:].view(-1)\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator):\n",
    "            # turn off teacher forcing\n",
    "            outputs = seq2seq(batch.src, batch.trg, teacher_forcing_ratio=0) \n",
    "\n",
    "            # trg = [trg sent len, batch size]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch.trg[1:].view(-1)\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)    \n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 30\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(seq2seq, train_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(seq2seq, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'tut2-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
