{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1665dc1cf70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "SEED = 2222\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "annotator = VnCoreNLP(\"VnCoreNLP-master\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', 'put', 'the', 'dustpan', 'in', 'the', 'broom', 'closet']\n",
      "['Cuốn', 'sách', 'này', 'là', 'của', 'tôi', '.', 'Của', 'bạn', 'đâu', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "def tokenize_en(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "def tokenize_vi(text):\n",
    "    return [tok for tok in itertools.chain.from_iterable(annotator.tokenize(text))]\n",
    "\n",
    "text_en = 'Please put the dustpan in the broom closet'\n",
    "text_vi = 'Cuốn sách này là của tôi. Của bạn đâu?'\n",
    "print(tokenize_en(text_en))\n",
    "print(tokenize_vi(text_vi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nlp import load_dataset\n",
    "\n",
    "\n",
    "# Define the fields for your data\n",
    "fields = {\"English\": (\"src\", tokenize_en), \"Vietnamese\": (\"trg\", tokenize_vi)}\n",
    "\n",
    "def create_raw_dataset():\n",
    "    data_dir = \"\"\n",
    "    en_sents = open(data_dir + 'english.txt', \"r\",encoding=\"utf-8\" ).read().splitlines()\n",
    "    vi_sents = open(data_dir + 'vietnamese.txt', \"r\" ,encoding=\"utf-8\").read().splitlines()\n",
    "    return {\n",
    "        \"English\": [line for line in en_sents[:5000]],\n",
    "        \"Vietnamese\": [line for line in vi_sents[:5000]],\n",
    "    }\n",
    "raw_data = create_raw_dataset()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=[\"English\", \"Vietnamese\"])\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.125)\n",
    "\n",
    "train.to_json(\"train.json\", orient=\"records\", lines=True)\n",
    "test.to_json(\"test.json\", orient=\"records\", lines=True)\n",
    "val.to_json(\"val.json\", orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default-0f5e6a7283a4ff1f (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to C:\\Users\\16262\\.cache\\huggingface\\datasets\\json\\default-0f5e6a7283a4ff1f\\0.0.0\\37a0541357cf138f0cfbe7b0fe461217a57444a79c645ea89529c7bec8b1c45c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            \r"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\16262\\\\.cache\\\\huggingface\\\\datasets\\\\json\\\\default-0f5e6a7283a4ff1f\\\\0.0.0\\\\37a0541357cf138f0cfbe7b0fe461217a57444a79c645ea89529c7bec8b1c45c.incomplete\\\\json-train.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:424\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     \u001b[39myield\u001b[39;00m tmp_dir\n\u001b[0;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dirname):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:462\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[1;32m--> 462\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[0;32m    463\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager, verify_infos\u001b[39m=\u001b[39;49mverify_infos, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs\n\u001b[0;32m    464\u001b[0m     )\n\u001b[0;32m    465\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:537\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    536\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> 537\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[0;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:865\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator)\u001b[0m\n\u001b[0;32m    864\u001b[0m generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_tables(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generator\u001b[39m.\u001b[39mgen_kwargs)\n\u001b[1;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;00m key, table \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mtqdm(generator, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m tables\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    866\u001b[0m     writer\u001b[39m.\u001b[39mwrite_table(table)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\datasets\\json\\37a0541357cf138f0cfbe7b0fe461217a57444a79c645ea89529c7bec8b1c45c\\json.py:65\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 65\u001b[0m     dataset \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m     67\u001b[0m \u001b[39m# We keep only the field we are interested in\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39ma JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[0;32m    294\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[0;32m    295\u001b[0m     parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[0;32m    296\u001b[0m     parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 135)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the data from the json files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, field\u001b[39m=\u001b[39;49mfields)\n\u001b[0;32m      3\u001b[0m test_data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m, data_files\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest.json\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, field\u001b[39m=\u001b[39mfields)\n\u001b[0;32m      4\u001b[0m val_data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m, data_files\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval.json\u001b[39m\u001b[39m\"\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, field\u001b[39m=\u001b[39mfields)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\load.py:548\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m    537\u001b[0m     cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m    538\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_kwargs,\n\u001b[0;32m    545\u001b[0m )\n\u001b[0;32m    547\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m    549\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config, download_mode\u001b[39m=\u001b[39;49mdownload_mode, ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[0;32m    550\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m    553\u001b[0m ds \u001b[39m=\u001b[39m builder_instance\u001b[39m.\u001b[39mas_dataset(split\u001b[39m=\u001b[39msplit, ignore_verifications\u001b[39m=\u001b[39mignore_verifications)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:449\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m    443\u001b[0m         dl_manager\u001b[39m.\u001b[39mmanual_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mThe dataset \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m with config \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m requires manual data. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Please follow the manual download instructions: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Manual data can be loaded with `nlp.load_dataset(\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, data_dir=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<path/to/manual/data>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    445\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[39m# Create a tmp dir and rename to self._cache_dir on successful exit.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mwith\u001b[39;49;00m incomplete_dir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache_dir) \u001b[39mas\u001b[39;49;00m tmp_data_dir:\n\u001b[0;32m    450\u001b[0m     \u001b[39m# Temporarily assign _cache_dir to tmp_data_dir to avoid having to forward\u001b[39;49;00m\n\u001b[0;32m    451\u001b[0m     \u001b[39m# it to every sub function.\u001b[39;49;00m\n\u001b[0;32m    452\u001b[0m     \u001b[39mwith\u001b[39;49;00m utils\u001b[39m.\u001b[39;49mtemporary_assignment(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_cache_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m, tmp_data_dir):\n\u001b[0;32m    453\u001b[0m         \u001b[39m# Try to download the already prepared dataset files\u001b[39;49;00m\n\u001b[0;32m    454\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    153\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[0;32m    154\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen\u001b[39m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nlp\\builder.py:430\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare.<locals>.incomplete_dir\u001b[1;34m(dirname)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(tmp_dir):\n\u001b[1;32m--> 430\u001b[0m         shutil\u001b[39m.\u001b[39;49mrmtree(tmp_dir)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\shutil.py:759\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 759\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\shutil.py:622\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    620\u001b[0m             os\u001b[39m.\u001b[39munlink(fullname)\n\u001b[0;32m    621\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m             onerror(os\u001b[39m.\u001b[39;49munlink, fullname, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[0;32m    623\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     os\u001b[39m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m         os\u001b[39m.\u001b[39;49munlink(fullname)\n\u001b[0;32m    621\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m         onerror(os\u001b[39m.\u001b[39munlink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\16262\\\\.cache\\\\huggingface\\\\datasets\\\\json\\\\default-0f5e6a7283a4ff1f\\\\0.0.0\\\\37a0541357cf138f0cfbe7b0fe461217a57444a79c645ea89529c7bec8b1c45c.incomplete\\\\json-train.arrow'"
     ]
    }
   ],
   "source": [
    "# Load the data from the json files\n",
    "train_data = load_dataset(\"json\", data_files=\"train.json\", split=\"train\", field=fields)\n",
    "test_data = load_dataset(\"json\", data_files=\"test.json\", split=\"test\", field=fields)\n",
    "val_data = load_dataset(\"json\", data_files=\"val.json\", split=\"val\", field=fields)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
